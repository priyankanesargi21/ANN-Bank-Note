{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvaaQ-v2ONVM"
      },
      "source": [
        "# Backprop on the BankNote Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "import csv\n",
        " \n",
        "# Load a CSV file\n",
        "def loadCsv(filename):\n",
        "        trainSet = []\n",
        "        \n",
        "        lines = csv.reader(open(filename, 'r'))\n",
        "        dataset = list(lines)\n",
        "        for i in range(1,len(dataset)):\n",
        "                for j in range(4):\n",
        "                        # print(\"DATA {}\".format(dataset[i]))\n",
        "                        dataset[i][j] = float(dataset[i][j])\n",
        "                trainSet.append(dataset[i])\n",
        "        return trainSet\n",
        "        \n",
        "# Find the min and max values for each column\n",
        "\n",
        "def minmax(dataset):\n",
        "        minmax = list()\n",
        "        stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "        return stats\n",
        " \n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize(dataset, minmax):\n",
        "        for row in dataset:\n",
        "                for i in range(len(row)-1):\n",
        "                        row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        " \n",
        "# Convert string column to float\n",
        "def column_to_float(dataset, column):\n",
        "        for row in dataset:\n",
        "                try:\n",
        "                        row[column] = float(row[column])\n",
        "                except ValueError:\n",
        "                        print(\"Error with row\",column,\":\",row[column])\n",
        "                        pass\n",
        " \n",
        "# Convert string column to integer\n",
        "def column_to_int(dataset, column):\n",
        "        class_values = [row[column] for row in dataset]\n",
        "        unique = set(class_values)\n",
        "        lookup = dict()\n",
        "        for i, value in enumerate(unique):\n",
        "                lookup[value] = i\n",
        "        for row in dataset:\n",
        "                row[column] = lookup[row[column]]\n",
        "        return lookup\n",
        " \n",
        "\n",
        " \n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "        dataset_split = list()\n",
        "        dataset_copy = list(dataset)\n",
        "        fold_size = int(len(dataset) / n_folds)\n",
        "        for i in range(n_folds):\n",
        "                fold = list()\n",
        "                while len(fold) < fold_size:\n",
        "                        index = randrange(len(dataset_copy))\n",
        "                        fold.append(dataset_copy.pop(index))\n",
        "                dataset_split.append(fold)\n",
        "        return dataset_split\n",
        " \n",
        "# Calculate accuracy percentage\n",
        "def accuracy_met(actual, predicted):\n",
        "        correct = 0\n",
        "        for i in range(len(actual)):\n",
        "                if actual[i] == predicted[i]:\n",
        "                        correct += 1\n",
        "        return correct / float(len(actual)) * 100.0\n",
        " \n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def run_algorithm(dataset, algorithm, n_folds, *args):\n",
        "        folds = cross_validation_split(dataset, n_folds)\n",
        "        #for fold in folds:\n",
        "                #print(\"Fold {} \\n \\n\".format(fold))\n",
        "        scores = list()\n",
        "        for fold in folds:\n",
        "                #print(\"Test Fold {} \\n \\n\".format(fold))\n",
        "                train_set = list(folds)\n",
        "                train_set.remove(fold)\n",
        "                train_set = sum(train_set, [])\n",
        "                test_set = list()\n",
        "                for row in fold:\n",
        "                        row_copy = list(row)\n",
        "                        test_set.append(row_copy)\n",
        "                        row_copy[-1] = None\n",
        "                predicted = algorithm(train_set, test_set, *args)\n",
        "                actual = [row[-1] for row in fold]\n",
        "                accuracy = accuracy_met(actual, predicted)\n",
        "                cm = confusion_matrix(actual, predicted)\n",
        "                print('\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in cm]))\n",
        "                #confusionmatrix = np.matrix(cm)\n",
        "                FP = cm.sum(axis=0) - np.diag(cm)\n",
        "                FN = cm.sum(axis=1) - np.diag(cm)\n",
        "                TP = np.diag(cm)\n",
        "                TN = cm.sum() - (FP + FN + TP)\n",
        "                print('False Positives\\n {}'.format(FP))\n",
        "                print('False Negetives\\n {}'.format(FN))\n",
        "                print('True Positives\\n {}'.format(TP))\n",
        "                print('True Negetives\\n {}'.format(TN))\n",
        "                TPR = TP/(TP+FN)\n",
        "                print('Sensitivity \\n {}'.format(TPR))\n",
        "                TNR = TN/(TN+FP)\n",
        "                print('Specificity \\n {}'.format(TNR))\n",
        "                Precision = TP/(TP+FP)\n",
        "                print('Precision \\n {}'.format(Precision))\n",
        "                Recall = TP/(TP+FN)\n",
        "                print('Recall \\n {}'.format(Recall))\n",
        "                Acc = (TP+TN)/(TP+TN+FP+FN)\n",
        "                print('Áccuracy \\n{}'.format(Acc))\n",
        "                Fscore = 2*(Precision*Recall)/(Precision+Recall)\n",
        "                print('FScore \\n{}'.format(Fscore))\n",
        "                k=cohen_kappa_score(actual, predicted)\n",
        "                print('Çohen Kappa \\n{}'.format(k))\n",
        "                scores.append(accuracy)\n",
        "        return scores\n",
        " \n",
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "        activation = weights[-1]\n",
        "        for i in range(len(weights)-1):\n",
        "                activation += weights[i] * inputs[i]\n",
        "        return activation\n",
        " \n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "        return 1.0 / (1.0 + exp(-activation))\n",
        " \n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "        inputs = row\n",
        "        for layer in network:\n",
        "                new_inputs = []\n",
        "                for neuron in layer:\n",
        "                        activation = activate(neuron['weights'], inputs)\n",
        "                        neuron['output'] = transfer(activation)\n",
        "                        new_inputs.append(neuron['output'])\n",
        "                inputs = new_inputs\n",
        "        return inputs\n",
        " \n",
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "        return output * (1.0 - output)\n",
        " \n",
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "        for i in reversed(range(len(network))):\n",
        "                layer = network[i]\n",
        "                errors = list()\n",
        "                if i != len(network)-1:\n",
        "                        for j in range(len(layer)):\n",
        "                                error = 0.0\n",
        "                                for neuron in network[i + 1]:\n",
        "                                        error += (neuron['weights'][j] * neuron['delta'])\n",
        "                                errors.append(error)\n",
        "                else:\n",
        "                        for j in range(len(layer)):\n",
        "                                neuron = layer[j]\n",
        "                                errors.append(expected[j] - neuron['output'])\n",
        "                for j in range(len(layer)):\n",
        "                        neuron = layer[j]\n",
        "                        neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        " \n",
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "        for i in range(len(network)):\n",
        "                inputs = row[:-1]                \n",
        "                if i != 0:\n",
        "                        inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "                for neuron in network[i]:\n",
        "                        for j in range(len(inputs)):\n",
        "                                temp = l_rate * neuron['delta'] * inputs[j] + mu * neuron['prev'][j]\n",
        "                                \n",
        "                                neuron['weights'][j] += temp\n",
        "                                #print(\"neuron weight{} \\n\".format(neuron['weights'][j]))\n",
        "                                neuron['prev'][j] = temp\n",
        "                        temp = l_rate * neuron['delta'] + mu * neuron['prev'][-1]\n",
        "                        neuron['weights'][-1] += temp\n",
        "                        neuron['prev'][-1] = temp\n",
        "                                \n",
        " \n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "        for epoch in range(n_epoch):\n",
        "                for row in train:\n",
        "                        outputs = forward_propagate(network, row)\n",
        "                        #print(network)\n",
        "                        expected = [0 for i in range(n_outputs)]\n",
        "                        expected[row[-1]] = 1\n",
        "                        #print(\"expected row{}\\n\".format(expected))\n",
        "                        backward_propagate_error(network, expected)\n",
        "                        update_weights(network, row, l_rate)\n",
        " \n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "        network = list()\n",
        "        hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]        \n",
        "        network.append(hidden_layer)\n",
        "        # hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]        \n",
        "        # network.append(hidden_layer)\n",
        "        output_layer = [{'weights':[random() for i in range(n_hidden + 1)],'prev':[0 for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "        network.append(output_layer)\n",
        "        print(network)\n",
        "        return network\n",
        " \n",
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "        outputs = forward_propagate(network, row)\n",
        "        return outputs.index(max(outputs))\n",
        " \n",
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "        n_inputs = len(train[0]) - 1\n",
        "        n_outputs = len(set([row[-1] for row in train]))\n",
        "        network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "        train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "        #print(\"network {}\\n\".format(network))\n",
        "        predictions = list()\n",
        "        for row in test:\n",
        "                prediction = predict(network, row)\n",
        "                predictions.append(prediction)\n",
        "        return(predictions)\n",
        " "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "l2ePk-RsOkzx",
        "outputId": "7ff12865-9c87-4567-9ce4-9b79595e2454"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b27ce42e-6fea-40c8-bd2b-8e54e9362c95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b27ce42e-6fea-40c8-bd2b-8e54e9362c95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving BankNote_Authentication 2.csv to BankNote_Authentication 2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRS4A4LSO4sj"
      },
      "source": [
        "import pandas as pd\n",
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'BankNote_Authentication 2.csv'\n",
        "dataset = loadCsv(filename)\n",
        "dataset\n",
        "# dataset = pd.read_csv(\"Iris.csv\")\n",
        "for i in range(len(dataset[0])-1):\n",
        "        column_to_float(dataset, i)\n",
        "# # convert class column to integers\n",
        "column_to_int(dataset, len(dataset[0])-1)\n",
        "# normalize input variables\n",
        "minmax = minmax(dataset)\n",
        "normalize(dataset, minmax)\n",
        "# evaluate algorithm\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KUmpwMCpP3qp",
        "outputId": "4f1215e3-a2cf-4c51-d8d7-dd69a092981b"
      },
      "source": [
        "n_folds = 5\n",
        "l_rate = 0.1\n",
        "mu=0.001\n",
        "n_epoch = 1500\n",
        "n_hidden = 4\n",
        "scores = run_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 149   0\n",
            "   0 125\n",
            "False Positives\n",
            " [0 0]\n",
            "False Negetives\n",
            " [0 0]\n",
            "True Positives\n",
            " [149 125]\n",
            "True Negetives\n",
            " [125 149]\n",
            "Sensitivity \n",
            " [1. 1.]\n",
            "Specificity \n",
            " [1. 1.]\n",
            "Precision \n",
            " [1. 1.]\n",
            "Recall \n",
            " [1. 1.]\n",
            "Áccuracy \n",
            "[1. 1.]\n",
            "FScore \n",
            "[1. 1.]\n",
            "Çohen Kappa \n",
            "1.0\n",
            " 145   5\n",
            "   1 123\n",
            "False Positives\n",
            " [1 5]\n",
            "False Negetives\n",
            " [5 1]\n",
            "True Positives\n",
            " [145 123]\n",
            "True Negetives\n",
            " [123 145]\n",
            "Sensitivity \n",
            " [0.96666667 0.99193548]\n",
            "Specificity \n",
            " [0.99193548 0.96666667]\n",
            "Precision \n",
            " [0.99315068 0.9609375 ]\n",
            "Recall \n",
            " [0.96666667 0.99193548]\n",
            "Áccuracy \n",
            "[0.97810219 0.97810219]\n",
            "FScore \n",
            "[0.97972973 0.97619048]\n",
            "Çohen Kappa \n",
            "0.9559296590177997\n",
            " 166   0\n",
            "   0 108\n",
            "False Positives\n",
            " [0 0]\n",
            "False Negetives\n",
            " [0 0]\n",
            "True Positives\n",
            " [166 108]\n",
            "True Negetives\n",
            " [108 166]\n",
            "Sensitivity \n",
            " [1. 1.]\n",
            "Specificity \n",
            " [1. 1.]\n",
            "Precision \n",
            " [1. 1.]\n",
            "Recall \n",
            " [1. 1.]\n",
            "Áccuracy \n",
            "[1. 1.]\n",
            "FScore \n",
            "[1. 1.]\n",
            "Çohen Kappa \n",
            "1.0\n",
            " 143   0\n",
            "   0 131\n",
            "False Positives\n",
            " [0 0]\n",
            "False Negetives\n",
            " [0 0]\n",
            "True Positives\n",
            " [143 131]\n",
            "True Negetives\n",
            " [131 143]\n",
            "Sensitivity \n",
            " [1. 1.]\n",
            "Specificity \n",
            " [1. 1.]\n",
            "Precision \n",
            " [1. 1.]\n",
            "Recall \n",
            " [1. 1.]\n",
            "Áccuracy \n",
            "[1. 1.]\n",
            "FScore \n",
            "[1. 1.]\n",
            "Çohen Kappa \n",
            "1.0\n",
            " 153   0\n",
            "   0 121\n",
            "False Positives\n",
            " [0 0]\n",
            "False Negetives\n",
            " [0 0]\n",
            "True Positives\n",
            " [153 121]\n",
            "True Negetives\n",
            " [121 153]\n",
            "Sensitivity \n",
            " [1. 1.]\n",
            "Specificity \n",
            " [1. 1.]\n",
            "Precision \n",
            " [1. 1.]\n",
            "Recall \n",
            " [1. 1.]\n",
            "Áccuracy \n",
            "[1. 1.]\n",
            "FScore \n",
            "[1. 1.]\n",
            "Çohen Kappa \n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_Jn34q3WYI7V",
        "outputId": "0de1b047-97e7-47bc-cb5e-039514434c47"
      },
      "source": [
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [100.0, 97.8102189781022, 100.0, 100.0, 100.0]\n",
            "Mean Accuracy: 99.562%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmiTVh5nPBRS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}